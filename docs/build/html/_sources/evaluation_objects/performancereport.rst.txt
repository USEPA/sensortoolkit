******************************************************
Creating Testing Reports - The PerformanceReport Class
******************************************************

.. role:: raw-html(raw)
  :format: html
  
Creating testing reports with ``sensortoolkit.PerformanceReport`` is very similar to using
the ``sensortoolkit.SensorEvaluation`` class. Users pass the same attributes to the
``PerformanceReport`` class as they do to conduct analysis with ``SensorEvaluation``
(``PerformanceReport`` is an inherited class of ``SensorEvaluation``, which means that
its initialization creates a ``SensorEvaluation`` instance and is used to construct reports).

There are a few additional attributes users can pass to the ``PerformanceReport``
class, including the dictionaries ``testing_org`` and ``testing_loc``. These
dictionaries house information about the testing organization, contact information,
and site details including the address, coordinates, and site AQS ID if applicable.

Below is an example of running the `PerformanceReport` class to create a testing
report for the ``Example_Make_Model`` sensor evaluating its :raw-html:`PM<sub>2.5</sub>` performance.

.. code-block:: python

  pollutant = sensortoolkit.Parameter('PM25')

  # Instantiate the PerformanceReport class for the example sensor dataset
  report = sensortoolkit.PerformanceReport(sensor_object,
                                           pollutant,
                                           reference_object,
                                           write_to_file=True,
                                           figure_search=False,
                                           testing_loc=loc,
                                           testing_org=org)

  # Compile the report and save the file to the reports subfolder
  report.CreateReport()

Arguments passed to ``PerformanceReport``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. list-table:: ``sensortoolkit.PerformanceReport()`` attributes
  :widths: 50 75
  :header-rows: 1

  * - Attribute name
    - Description
  * - ``sensor``
    - A ``sensortoolkit.AirSensor`` object.
  * - ``param``
    - A ``sensortoolkit.Parameter`` object.
  * - ``reference``
    - A ``sensortoolkit.ReferenceMonitor`` object.
  * - ``write_to_file``
    - If true, evaluation statistics are saved to ``/data/eval_stats/[name of sensor]`` |br|
      and figures are written to ``/figures/[name of sensor]``.
  * - ``figure_search``
    - If true, PerformanceReport will search for figures in the ``/figures`` |br|
      directory before attempting to create new figures. If false, |br|
      PerformanceReport will create all new figures (may risk overwriting |br|
      existing figures). Defaults to False.
  * - ``**kwargs``
    - ``testing_org`` and ``testing_loc``. See `Keyword arguments passed to sensorevaluation <../evaluation_objects/sensoreval/index.html#keyword-arguments-passed-to-sensorevaluation>`_.

Console Output
^^^^^^^^^^^^^^

When the above code block is run (including both instantiation of the ``PerformanceReport``
object ``report`` and running the ``PerformanceReport.create_report()`` method),
the following will be printed to the console.

.. code-block:: console

  Computing normalized PM25 values (by T-API T640X at 16.67 LPM)
  Computing normalized PM25 values (by T-API T640X at 16.67 LPM)
  Computing mean parameter values across concurrent sensor datasets
  Computing mean parameter values across concurrent sensor datasets
  Populating deployment dataframe with evaluation statistics
  Computing CV for 1-Hour averaged PM25
  ..N excluded: 20 out of 756 total
  ..N concurrent: 736
  ..Concurrent measurement timeframe: 2019-08-01 13:00:00+00:00 - 2019-09-02 00:00:00+00:00
  Computing CV for 24-Hour averaged PM25
  ..N excluded: 2 out of 32 total
  ..N concurrent: 30
  ..Concurrent measurement timeframe: 2019-08-02 00:00:00+00:00 - 2019-09-01 00:00:00+00:00
  Computing 1-hour regression statistics for Example_Make_Model vs. T-API T640X at 16.67 LPM
  ..SN01
  ..SN02
  ..SN03
  Computing 24-hour regression statistics for Example_Make_Model vs. T-API T640X at 16.67 LPM
  ..SN01
  ..SN02
  ..SN03
  ..Saving dataset to the following path: C:\Users\...\Documents\sensortoolkit_testing\data\eval_stats\Example_Make_Model\Example_Make_Model_PM25_vs_T-API_T640X_at_16.67_LPM_stats_df_211102.csv
  Computing 1-hour regression statistics for Example_Make_Model vs. T-API T640X at 16.67 LPM
  ..SN01
  ..SN02
  ..SN03
  Computing 24-hour regression statistics for Example_Make_Model vs. T-API T640X at 16.67 LPM
  ..SN01
  ..SN02
  ..SN03
  ..Saving dataset to the following path: C:\Users\...\Documents\sensortoolkit_testing\data\eval_stats\Example_Make_Model\Example_Make_Model_PM25_vs_T-API_T640X_at_16.67_LPM_stats_df_211102.csv
  Creating Testing Report for Example_Make_Model
  ..Adding figures to report
  ..creating subplot for 1 sensor with 1 row and 1 column
  ..creating subplot for 1 sensor with 1 row and 1 column
  ..Adding tabular data
  Computing normalized PM25 values (by T-API T640X at 16.67 LPM)
  Computing normalized PM25 values (by T-API T640X at 16.67 LPM)
  ..creating subplot for 3 sensors with 1 row and 3 columns
  ..creating subplot for 3 sensors with 1 row and 3 columns
  ..Saving report
  ....\reports\Example_Make_Model\PM25\Base_Testing_Report_PM25_Example_Make_Model_211102.pptx

.. note::

   Reports are saved within the ``/reports`` folder, which is located inside the
   directory pointed to by the project path.

Example Report
--------------

Below is an example report for the ``Example_Make_Model`` sensor.

Please note that at present, ``PerformanceReport`` does not populate the FRM/FEM
Instrumentation table with calibration dates, flowrate verification checks, and
any description of maintenance activities conducted on the reference instrumentation.
Users must either insert these details manually by placing information in the
provided table, or by appending documentation to the end of the report and noting on
the supplemental information page that addition documentation has been attached.

.. tabbed:: Page 1 - Testing Summary

  The first page of the testing report allows testers to insert information about their
  organization including contact information, and testers are also encouraged to
  provide details about the sensor and FRM/FEM instrumentation used for testing.

  Various plots generated via the ``PerformanceReport`` class are displayed below
  information about the deployment. These figures provide indication of the sensor's
  performance during the testing period, site conditions including temperature and
  relative humidity, and meteorological influences that may be present in sensor data.

  .. figure:: ../data/performance_report_example_pg1.png
     :align: center

     Example_Make_Model Base Testing Report (Page 1)

.. tabbed:: Page 2 - Tabular Statistics

  The second page of the report includes tabular statistics, such as the performance
  metric values characterizing sensor vs. FRM/FEM accuracy (bias and linearity),
  error, and sensor-sensor (intersensor) precision.

  .. figure:: ../data/performance_report_example_pg2.png
    :align: center

    Example_Make_Model Base Testing Report (Page 2)

.. tabbed:: Page 3 - Sensor vs. FRM/FEM Scatter

  Scatter plots for each sensor unit vs. FRM/FEM measurement pairs are displayed
  on a third page of the report.

  .. figure:: ../data/performance_report_example_pg3.png
     :align: center

     Example_Make_Model Base Testing Report (Page 3)

.. tabbed:: Page 4 - Supplemental Information

  .. figure:: ../data/performance_report_example_pg4.png
     :align: center

     Example_Make_Model Base Testing Report (Page 4)



.. |br| raw:: html

 <br />
